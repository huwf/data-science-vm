{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Hadoop and HDFS\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "* Import a spam dataset into HDFS filesystem\n",
    "* Run a MapReduce task using the Hadoop \"Streaming\" API (Python)\n",
    "* Run a machine learning algorithm using Mahout in order to create a simple spam filter using the naive Bayes algorithm.\n",
    "\n",
    "Remember:\n",
    "\n",
    "* If you are not on the wired network, you will need to connect to the VPN\n",
    "* You do not have to use Jupyter.  If you prefer, you can do everything in the Putty terminal.  However, if you do use Jupyter, you should bind the service to `0.0.0.0` on port `8888`, and add the token in as a password.\n",
    "\n",
    "## Start a new terminal\n",
    "In addition to using notebooks, other features of Jupyter include running a terminal.  On the main menu on the `Home` page, you can start a new terminal by clicking on `New` -> `Terminal`.  Do this now, so that you can run interactive bash commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Hadoop is running\n",
    "\n",
    "The first thing to do is to check that we have Hadoop installed and running.  Open a terminal, and type in: `hadoop version`, which should show you that the version you have is Hadoop 2.7.4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "\n",
    "The first thing we're going to do, is download some data.  This represents data which could be remote or in a datacentre somewhere.  Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# YOUR CODE HERE\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
    "unzip YouTube-Spam-Collection-v1.zip\n",
    "ls -lh *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "Having downloaded the data, we want to be able to run a Machine learning algorithm over it.  To do this, we will use the Hadoop Streaming API, which allows us to write Python code.  When we call Hadoop, we pass two Python files to the command - one which maps, and one which reduces.\n",
    "\n",
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head -n 10 Youtube04-Eminem.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word counting\n",
    "\n",
    "The first thing we want to do is to set up a MapReduce function which will allow us to count the number of each individual word from the `comment` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#TODO: Get rid of code below\n",
    "#!/usr/bin/env python\n",
    "# MAPPER\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "input_text = open('Youtube04-Eminem.csv', 'r')\n",
    "# When we move to the actual MapReduce job, we will need to read from STDIN\n",
    "# input_text = sys.stdin\n",
    "\n",
    "reader = csv.reader(input_text)\n",
    "# Skip the column header\n",
    "next(reader)\n",
    "for row in reader:\n",
    "    tokens = row[3].split(' ')\n",
    "    for t in tokens:\n",
    "        # print tab delimted here,\n",
    "        # will be input for the reducer\n",
    "        print('%s\\t%d' % (t, 1))    \n",
    "    \n",
    "\n",
    "input_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#TODO: Get rid of code below\n",
    "# REDUCER\n",
    "\n",
    "import sys\n",
    "# Keep simple example in for now, switch to stdin later\n",
    "# input_text = ['+447935454150', 'lovely', 'girl', 'talk', 'to', 'me', 'xxx\\ufeff']\n",
    "\n",
    "input_text = [\n",
    "    '+447935454150\t1',\n",
    "    'lovely\t1',\n",
    "    'girl\t1',\n",
    "    'talk\t1',\n",
    "    'to\t1',\n",
    "    'me\t1',\n",
    "    'xxxï»¿\t1',\n",
    "    # Add an extra one to test that it works\n",
    "    'to\\t1'\n",
    "]\n",
    "\n",
    "# input_text = sys.stdin\n",
    "words = {}\n",
    "\n",
    "for line in input_text:\n",
    "    word, count = line.split('\\t', 1)\n",
    "#     print('word: %s count: %s' % (word, count))\n",
    "    \n",
    "    # Convert count to an integer\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # We can safely ignore, so keep calm and carry on\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    if word in words:\n",
    "        words[word] += 1\n",
    "    else:\n",
    "        words[word] = 1\n",
    "        \n",
    "for w in words:\n",
    "    print('%s\\t%s' % (w, words[w]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work in principle?  We can test without Hadoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat Youtube04-Eminem.csv | ./mapper.py | ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# start-dfs.sh\n",
    "# start-yarn.sh\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files mapper.py,reducer.py \\\n",
    "-input Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output output \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up HDFS\n",
    "\n",
    "There are a few more things you need to make Hadoop work nicely.  We are going to set up pseudo-distributed mode, which requires passwordless SSH to be set up.  To do this, we need to run the following commands:\n",
    "\n",
    "    ssh-keygen -t rsa -P ''\n",
    "    cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys\n",
    "    ssh localhost\n",
    "\n",
    "For the first command, leave all the options as default (press enter for each one to do this).  For the second, you are checking you are able to SSH into your own machine, localhost.  Normally this would be done to a different computer, but to SSH into localhost.  You will have output like the following text.  Type `yes`, because you do still want to connect.\n",
    "\n",
    "\n",
    "    The authenticity of host 'localhost (127.0.0.1)' can't be established.\n",
    "    ECDSA key fingerprint is 18:6e:42:bd:0c:8c:35:bc:d9:e8:3c:c6:a3:08:56:43.\n",
    "    Are you sure you want to continue connecting (yes/no)? yes\n",
    "    \n",
    "Type `exit` from that internal shell, and then try and ssh to `0.0.0.0`.  We also need to make sure that this works for some of the tasks ahead.\n",
    "\n",
    "    ssh 0.0.0.0\n",
    "    \n",
    "Now, we need to set up some configuration for the various parts of Hadoop.  Firstly, download the following file, and make sure it has permissions to be executed.  It's good practice to check what unknown files from the Internet are doing, so have a read of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm hadoop-config*\n",
    "wget https://raw.githubusercontent.com/huwf/data-science-vm/master/hadoop-config.sh\n",
    "cat hadoop-config.sh\n",
    "chmod 755 hadoop-config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal, and run the following command which will execute the file: **YOU WILL NEED TO RUN THIS AS SUDO**\n",
    "\n",
    "    sudo ./hadoop-config.sh\n",
    "\n",
    "We should now have HDFS configured for pseudo-distributed mode.  We will now need to create a disk for HDFS, which will use the configurations we just set:\n",
    "\n",
    "    hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting services\n",
    "\n",
    "Now we need to start the different services and we can get to work!  Run the following command in the terminal to start DFS:\n",
    "\n",
    "    start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this has left you with, you can see the processes which are running on the JVM by running the `jps` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a HDFS disk, and the appropriate Hadoop services running, we can start to import the data into the new HDFS filesystem and run the MapReduce task there.  This is allowing us to move from the single mode to pseudo-distributed mode.  Following these principles, we could apply MapReduce over various machines.\n",
    "\n",
    "In order to achieve this, we need to: \n",
    "\n",
    "* Create a directory for the input\n",
    "* Import the data from the local file to the HDFS datanode\n",
    "* Run the MapReduce job\n",
    "* View the output\n",
    "\n",
    "The Commands on HDFS are similar to standard linux CLI commands, except for the fact that they are prefixed by either `hadoop fs` or `hdfs dfs`.\n",
    "\n",
    "The `hadoop fs` command is more general, as it can cope with different types of filesystem, such as the one on the local disk.  As such, this is a better choice to use for commands relating solely to HDFS.\n",
    "\n",
    "The command to create a directory is `-mkdir`.  Create a directory `/input` on the HDFS system.  Use the `hdfs dfs` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE\n",
    "# hadoop fs -mkdir /input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing our data, we are dealing with two different filesystems: the local system and the HDFS node so we will use `hadoop fs`, with the `-copyFromLocal` command.  The next two arguments are file source and destination.\n",
    "\n",
    "HDFS filesystems are defined by a URI prefixed by `hdfs://`, and the `hdfs dfs` and `hadoop fs` commands will normally expect to see them.\n",
    "\n",
    "If they are not specified, the default location of the filesystem is specified in `core-site.xml`, which is one of the config files we imported earlier.  The value can be seen from the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cat $HADOOP_HOME/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the `copyFromLocal` we can either specify `hdfs://localhost:9000` or leave it out.  The local file can be specified with a relative command, leaving the import command as one of the following two.  Pick one and execute it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# With fully specified URI\n",
    "hadoop fs -copyFromLocal *.csv hdfs://localhost:9000/input\n",
    "# Implied URI based on default\n",
    "# hadoop fs -copyFromLocal *.csv /input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same for the `mapper.py` and `reducer.py` files we created for the MapReduce task earlier, keeping those in the `input` directory as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE\n",
    "hadoop fs -copyFromLocal mapper.py /input/mapper.py\n",
    "hadoop fs -copyFromLocal reducer.py /input/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the terminal, run the command again, this time over all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17/11/30 11:23:01 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "17/11/30 11:23:01 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "17/11/30 11:23:01 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "17/11/30 11:23:02 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "17/11/30 11:23:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/11/30 11:23:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local197003866_0001\n",
      "17/11/30 11:23:02 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "17/11/30 11:23:02 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "17/11/30 11:23:02 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "17/11/30 11:23:02 INFO mapreduce.Job: Running job: job_local197003866_0001\n",
      "17/11/30 11:23:02 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "17/11/30 11:23:02 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "17/11/30 11:23:02 INFO mapred.LocalJobRunner: Starting task: attempt_local197003866_0001_m_000000_0\n",
      "17/11/30 11:23:02 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "17/11/30 11:23:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/input/Youtube04-Eminem.csv:0+82896\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: PipeMapRed exec [/home/user/data-science-vm/././mapper.py]\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "17/11/30 11:23:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: Records R/W=454/1\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "17/11/30 11:23:03 INFO mapred.LocalJobRunner: \n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Starting flush of map output\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Spilling map output\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: bufstart = 0; bufend = 70736; bufvoid = 104857600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26177484(104709936); length = 36913/6553600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Finished spill 0\n",
      "17/11/30 11:23:03 INFO mapred.Task: Task:attempt_local197003866_0001_m_000000_0 is done. And is in the process of committing\n",
      "17/11/30 11:23:03 INFO mapred.LocalJobRunner: Records R/W=454/1\n",
      "17/11/30 11:23:03 INFO mapred.Task: Task 'attempt_local197003866_0001_m_000000_0' done.\n",
      "17/11/30 11:23:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local197003866_0001_m_000000_0\n",
      "17/11/30 11:23:03 INFO mapred.LocalJobRunner: Starting task: attempt_local197003866_0001_m_000001_0\n",
      "17/11/30 11:23:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "17/11/30 11:23:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/input/Youtube05-Shakira.csv:0+72706\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "17/11/30 11:23:03 INFO mapreduce.Job: Job job_local197003866_0001 running in uber mode : false\n",
      "17/11/30 11:23:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: PipeMapRed exec [/home/user/data-science-vm/././mapper.py]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: Records R/W=371/1\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "17/11/30 11:23:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "17/11/30 11:23:03 INFO mapred.LocalJobRunner: \n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Starting flush of map output\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Spilling map output\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: bufstart = 0; bufend = 55894; bufvoid = 104857600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26185308(104741232); length = 29089/6553600\n",
      "17/11/30 11:23:03 INFO mapred.MapTask: Finished spill 0\n",
      "17/11/30 11:23:04 INFO mapred.Task: Task:attempt_local197003866_0001_m_000001_0 is done. And is in the process of committing\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: Records R/W=371/1\n",
      "17/11/30 11:23:04 INFO mapred.Task: Task 'attempt_local197003866_0001_m_000001_0' done.\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local197003866_0001_m_000001_0\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: Starting task: attempt_local197003866_0001_r_000000_0\n",
      "17/11/30 11:23:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "17/11/30 11:23:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "17/11/30 11:23:04 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6a6d71a1\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "17/11/30 11:23:04 INFO reduce.EventFetcher: attempt_local197003866_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "17/11/30 11:23:04 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local197003866_0001_m_000000_0 decomp: 89198 len: 89202 to MEMORY\n",
      "17/11/30 11:23:04 INFO reduce.InMemoryMapOutput: Read 89198 bytes from map-output for attempt_local197003866_0001_m_000000_0\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 89198, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->89198\n",
      "17/11/30 11:23:04 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local197003866_0001_m_000001_0 decomp: 70444 len: 70448 to MEMORY\n",
      "17/11/30 11:23:04 INFO reduce.InMemoryMapOutput: Read 70444 bytes from map-output for attempt_local197003866_0001_m_000001_0\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 70444, inMemoryMapOutputs.size() -> 2, commitMemory -> 89198, usedMemory ->159642\n",
      "17/11/30 11:23:04 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: 2 / 2 copied.\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs\n",
      "17/11/30 11:23:04 INFO mapred.Merger: Merging 2 sorted segments\n",
      "17/11/30 11:23:04 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 159636 bytes\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: Merged 2 segments, 159642 bytes to disk to satisfy reduce memory limit\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: Merging 1 files, 159644 bytes from disk\n",
      "17/11/30 11:23:04 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "17/11/30 11:23:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "17/11/30 11:23:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 159637 bytes\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: 2 / 2 copied.\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: PipeMapRed exec [/home/user/data-science-vm/././reducer.py]\n",
      "17/11/30 11:23:04 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "17/11/30 11:23:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: Records R/W=16502/1\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "17/11/30 11:23:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "17/11/30 11:23:04 INFO mapred.Task: Task:attempt_local197003866_0001_r_000000_0 is done. And is in the process of committing\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: 2 / 2 copied.\n",
      "17/11/30 11:23:04 INFO mapred.Task: Task attempt_local197003866_0001_r_000000_0 is allowed to commit now\n",
      "17/11/30 11:23:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_local197003866_0001_r_000000_0' to hdfs://localhost:9000/output_8/_temporary/0/task_local197003866_0001_r_000000\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: Records R/W=16502/1 > reduce\n",
      "17/11/30 11:23:04 INFO mapred.Task: Task 'attempt_local197003866_0001_r_000000_0' done.\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local197003866_0001_r_000000_0\n",
      "17/11/30 11:23:04 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "17/11/30 11:23:05 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/11/30 11:23:05 INFO mapreduce.Job: Job job_local197003866_0001 completed successfully\n",
      "17/11/30 11:23:05 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=793888\n",
      "\t\tFILE: Number of bytes written=1916428\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=394100\n",
      "\t\tHDFS: Number of bytes written=40335\n",
      "\t\tHDFS: Number of read operations=28\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=5\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=825\n",
      "\t\tMap output records=16502\n",
      "\t\tMap output bytes=126630\n",
      "\t\tMap output materialized bytes=159650\n",
      "\t\tInput split bytes=201\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3872\n",
      "\t\tReduce shuffle bytes=159650\n",
      "\t\tReduce input records=16502\n",
      "\t\tReduce output records=3867\n",
      "\t\tSpilled Records=33004\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=950009856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=155602\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=40335\n",
      "17/11/30 11:23:05 INFO streaming.StreamJob: Output directory: hdfs://localhost:9000/output_8\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-input hdfs://localhost:9000/input/Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output hdfs://localhost:9000/output_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 user supergroup          0 2017-11-30 11:02 /output_7/_SUCCESS\n",
      "-rw-r--r--   1 user supergroup      25580 2017-11-30 11:02 /output_7/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -ls /output_7\n",
    "# hdfs dfs -cat output_7/_SUCCESS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_SUCCCESS` file indicates that the job was a success, which is good.  The other file, `part-00000` contains the result.  Write code in the cell below to get the output (from HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can view this \n",
    "\n",
    "You can include multiple `-input` parameters to operate on more than one file.  Update the streaming command above to include all 5 files in the cell below.  Make sure you include a new output directory!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Update this command to include multiple files\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-input hdfs://localhost:9000/input/Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output hdfs://localhost:9000/output_8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
