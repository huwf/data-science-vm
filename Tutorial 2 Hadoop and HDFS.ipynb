{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Hadoop and HDFS\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "* Import a spam dataset into HDFS filesystem\n",
    "* Run a MapReduce task using the Hadoop \"Streaming\" API (Python)\n",
    "* Run a machine learning algorithm using Mahout in order to create a simple spam filter using the naive Bayes algorithm.\n",
    "\n",
    "Remember:\n",
    "\n",
    "* If you are not on the wired network, you will need to connect to the VPN\n",
    "* You do not have to use Jupyter.  If you prefer, you can do everything in the Putty terminal.  However, if you do use Jupyter, you should bind the service to `0.0.0.0` on port `8888`, and add the token in as a password.\n",
    "\n",
    "## Start a new terminal\n",
    "In addition to using notebooks, other features of Jupyter include running a terminal.  On the main menu on the `Home` page, you can start a new terminal by clicking on `New` -> `Terminal`.  Do this now, so that you can run interactive bash commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Hadoop is running\n",
    "\n",
    "The first thing to do is to check that we have Hadoop installed and running.  Open a terminal, and type in: `hadoop version`, which should show you that the version you have is Hadoop 2.7.4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# YOUR CODE HERE\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\n",
    "unzip YouTube-Spam-Collection-v1.zip\n",
    "ls -lh *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "Having downloaded the data, we want to be able to run a Machine learning algorithm over it.  To do this, we will use the Hadoop Streaming API, which allows us to write Python code.  When we call Hadoop, we pass two Python files to the command - one which maps, and one which reduces.\n",
    "\n",
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "head -n 10 Youtube04-Eminem.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word counting\n",
    "\n",
    "The first thing we want to do is to set up a MapReduce function which will allow us to count the number of each individual word from the `comment` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPPER\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "input_text = open('Youtube04-Eminem.csv', 'r')\n",
    "# When we move to the actual MapReduce job, we will need to read from STDIN\n",
    "input_text = sys.stdin\n",
    "\n",
    "reader = csv.reader(input_text)\n",
    "# Skip the column header\n",
    "next(reader)\n",
    "for row in reader:\n",
    "    tokens = row[3].split(' ')\n",
    "    print(tokens)\n",
    "    for t in tokens:\n",
    "        print\n",
    "        # print tab delimted here,\n",
    "        # will be input for the reducer\n",
    "        print('%s\\t%d' % (t, 1))    \n",
    "    # Only do it for the first record for now\n",
    "    break\n",
    "\n",
    "# input_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCER\n",
    "\n",
    "import sys\n",
    "# Keep simple example in for now, switch to stdin later\n",
    "# input_text = ['+447935454150', 'lovely', 'girl', 'talk', 'to', 'me', 'xxx\\ufeff']\n",
    "\n",
    "input_text = [\n",
    "    '+447935454150\t1',\n",
    "    'lovely\t1',\n",
    "    'girl\t1',\n",
    "    'talk\t1',\n",
    "    'to\t1',\n",
    "    'me\t1',\n",
    "    'xxxï»¿\t1'\n",
    "]\n",
    "\n",
    "# input_text = sys.stdin\n",
    "words = {}\n",
    "\n",
    "for line in input_text:\n",
    "    word, count = line.split('\\t', 1)\n",
    "    print('word: %s count: %s' % (word, count))\n",
    "    \n",
    "    # Convert count to an integer\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # We can safely ignore, so keep calm and carry on\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    if word in words:\n",
    "        words[word] += 1\n",
    "    else:\n",
    "        words[word] = 1\n",
    "        \n",
    "for w in words:\n",
    "    print('%s\\t%s' % (w, words[w]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work in principle?  We can test without Hadoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cat Youtube04-Eminem.csv | ./mapper.py | ./reducer.py\n",
    "jps\n",
    "kill 11620\n",
    "kill 11783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up HDFS\n",
    "\n",
    "There are a few more things you need to make Hadoop work nicely.  We are going to set up pseudo-distributed mode, which requires passwordless SSH to be set up.  To do this, we need to run the following commands:\n",
    "\n",
    "    ssh-keygen -t rsa -P ''\n",
    "    ssh localhost\n",
    "    \n",
    "For the first command, leave all the options as default (press enter for each one to do this).  For the second, you are checking you are able to SSH into your own machine, localhost.  Normally this would be done to a different computer, but to SSH into localhost.  You will have output like the following text.  Type `yes`, because you do still want to connect.\n",
    "\n",
    "\n",
    "    The authenticity of host 'localhost (127.0.0.1)' can't be established.\n",
    "    ECDSA key fingerprint is 18:6e:42:bd:0c:8c:35:bc:d9:e8:3c:c6:a3:08:56:43.\n",
    "    Are you sure you want to continue connecting (yes/no)? yes\n",
    "    \n",
    "Now, we need to set up some configuration for the various parts of Hadoop.  Firstly, download the following file, and make sure it has permissions to be executed.  It's good practice to check what unknown files from the Internet are doing, so have a read of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm hadoop-config*\n",
    "wget https://raw.githubusercontent.com/huwf/data-science-vm/master/hadoop-config.sh\n",
    "cat hadoop-config.sh\n",
    "chmod 755 hadoop-config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal, and run the following command which will execute the file: **YOU WILL NEED TO RUN THIS AS SUDO**\n",
    "\n",
    "    sudo ./hadoop-config.sh\n",
    "\n",
    "We should now have HDFS configured for pseudo-distributed mode.  We will now need to create a disk for HDFS, which will use the configurations we just set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting services\n",
    "\n",
    "Now we need to start the different services and we can get to work!  Run the commands in the following cell to start YARN and DFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "start-dfs.sh\n",
    "echo \"started dfs\"\n",
    "start-yarn.sh\n",
    "echo \"started yarn\"\n",
    "# See what Hadoop (JVM) processes we have running on the VM\n",
    "jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# start-dfs.sh\n",
    "# start-yarn.sh\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-files ./mapper.py,./reducer.py \\\n",
    "-input Youtube04-Eminem.csv \\\n",
    "-mapper ./mapper.py \\\n",
    "-reducer ./reducer.py \\\n",
    "-output output \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
